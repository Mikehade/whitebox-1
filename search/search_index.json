{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Whitebox","text":"<p> Whitebox is an open source E2E ML monitoring platform with edge capabilities that plays nicely with kubernetes  </p> <p>Documentation:  https://whitebox-ai.github.io/whitebox/</p> <p>Source Code: https://github.com/whitebox-ai/whitebox</p> <p>Roadmap: https://github.com/whitebox-ai/whitebox/milestones</p> <p>Issue tracking https://github.com/orgs/whitebox-ai/projects/1/views/3</p> <p>Discord: https://discord.gg/bkAcsx4V</p> <p>Whitebox is a dynamic open source E2E ML monitoring platform with edge capabilities that plays nicely with kubernetes.</p> <p>The current key features are:</p> <ul> <li>Descriptive statistics</li> <li>Classification models evaluation metrics</li> <li>Data / Concept drift monitoring</li> <li>Explainable AI</li> <li>Alerts</li> </ul> <p>Design guidelines:</p> <ul> <li>Easy: Very easy to set up and get started with.</li> <li>Intuitive: Designed to be intuitive and easy to use.</li> <li>Pythonic SDK: Pythonic SDK for building your own monitoring infrastructure.</li> <li>Robust: Get production-ready MLOps system.</li> <li>Kubernetes: Get production-ready code. With automatic interactive documentation.</li> </ul>"},{"location":"features/","title":"Features","text":""},{"location":"features/#design-decisions","title":"Design decisions","text":"<ul> <li>Easy: Very easy to set up and get started with.</li> <li>Intuitive: Designed to be intuitive and easy to use.</li> <li>Pythonic SDK: Pythonic SDK for building your own monitoring infrastructure.</li> <li>Robust: Get production-ready MLOps system.</li> <li>Kubernetes: Get production-ready code. With automatic interactive documentation.</li> </ul>"},{"location":"features/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Whitebox provides a nice list of descriptive statistics of input dataset, making the overview of data easy.</p>"},{"location":"features/#models-metrics","title":"Models Metrics","text":""},{"location":"features/#classification-models","title":"Classification Models","text":"<p>Whitebox includes comprehensive metrics tracking for classification models. This allows users to easily evaluate the performance of their classification models and identify areas for improvement. Additionally, users can set custom thresholds for each metric to receive alerts when performance deviates from expected results.</p>"},{"location":"features/#regression-models","title":"Regression Models","text":"<p>Whitebox includes comprehensive metrics tracking for regression models. This allows users to easily evaluate the performance of their regression models and identify areas for improvement. Additionally, users can set custom thresholds for each metric to receive alerts when performance deviates from expected results.</p>"},{"location":"features/#data-concept-drift-monitoring","title":"Data / Concept Drift Monitoring","text":"<p>Whitebox includes monitoring for data and concept drift. This feature tracks changes in the distribution of the data used to train models and alerts users when significant changes occur. Additionally, it detects changes in the performance of deployed models and alerts users when significant drift is detected. This allows users to identify and address data and model drift early, reducing the risk of poor model performance.</p>"},{"location":"features/#explainable-ai","title":"Explainable AI","text":"<p>Whitebox includes model explaination also. The explainability performed through the explainability report which allows user to know anytime which feature had the most impact on model's prediction.</p>"},{"location":"features/#alerts","title":"Alerts","text":"<p>Whitebox includes an alerting system that allows users to set custom thresholds for metrics and receive notifications when performance deviates from expected results. These alerts can be delivered via email, SMS, or webhook, and can be customized to fit the needs of any organization. This allows users to quickly respond to changes in model performance and take action to improve results.</p>"},{"location":"metric-definitions/","title":"Glossary / Metric definitions","text":""},{"location":"metric-definitions/#descriptive-statistics","title":"Descriptive statistics","text":""},{"location":"metric-definitions/#missing-values","title":"Missing values","text":"<p>Missing values metric calculates the summary of the number of missing values per feature. Missing values include <code>NaN</code> in numeric arrays, <code>NaN</code> or <code>None</code> in object arrays and <code>NaT</code> in datetimelike.</p>"},{"location":"metric-definitions/#non-missing-values","title":"Non-Missing values","text":"<p>Non-Missing values metric calculates the summary of the number of non-missing values per feature. Non-Missing values are all values beside <code>NaN</code> for numeric arrays, <code>NaN</code> or <code>None</code> for object arrays and <code>NaT</code> for datetimelike.</p>"},{"location":"metric-definitions/#mean-or-average-value","title":"Mean or Average value","text":"<p>Returns the average value per feature excluding <code>NaN</code> and <code>null</code> values.</p>"},{"location":"metric-definitions/#minimum-value","title":"Minimum value","text":"<p>Returns the minimum value per feature.</p>"},{"location":"metric-definitions/#maximum-value","title":"Maximum value","text":"<p>Returns the maximum value per feature.</p>"},{"location":"metric-definitions/#summary","title":"Summary","text":"<p>Returns the summary of the values per feature. Excludes <code>NaN</code> and <code>null</code> values during calculations.</p>"},{"location":"metric-definitions/#standard-deviation","title":"Standard Deviation","text":"<p>Returns the sample standard deviation per feature normalized by N-1 excluding <code>NaN</code> and <code>null</code> values during calculations. Formula:</p>  \u03c3 = \\sqrt{\u03a3(x_i-\u03bc)^2 \\over \u039d-1}"},{"location":"metric-definitions/#variance","title":"Variance","text":"<p>Returns the unbiased variance per feature normalized by N-1 excluding <code>NaN</code> and <code>null</code> values during calculations. Formula:</p>  \u03c3^2 = {\u03a3(x_i-\u03bc)^2 \\over \u039d-1}   \u03c3^2 = {\u03a3(x_i-\u03bc)^2 \\over \u039d-1}"},{"location":"metric-definitions/#classification-evaluation-metrics","title":"Classification evaluation metrics","text":""},{"location":"metric-definitions/#confusion-matrix","title":"Confusion Matrix","text":"<p>Returns the number of TP, TN, FP and FN. In case of a <code>multi-class classification</code> returns the number of TP, TN, FP and FN per class.</p> <p>A typical example for <code>binary classification</code> could be seen below in which:</p> <ul> <li>20 observations were correctly classified as positive.</li> <li>10 observations were incorrectly classified as negative while they were actually positive.</li> <li>5 observations were incorrectly classified as positive while they were actually negative.</li> <li>75 observations were correctly classified as negative.</li> </ul> Predicted Positive Predicted Negative Actual Positive 20 (TP) 10 (FN) Actual Negative 5 (FP) 75 (TN) <p>A typical example for <code>multi-class classification</code> could be seen below in which:</p> <ul> <li>15 observations were correctly classified as Class A.</li> <li>5 observations were incorrectly classified as Class B while they were actually Class A.</li> <li>2 observations were incorrectly classified as Class C while they were actually Class A.</li> <li>4 observations were incorrectly classified as Class A while they were actually Class B.</li> <li>20 observations were correctly classified as Class B.</li> <li>3 observations were incorrectly classified as Class C while they were actually Class B.</li> <li>2 observations were incorrectly classified as Class A while they were actually Class C.</li> <li>8 observations were incorrectly classified as Class B while they were actually Class C.</li> <li>25 observations were correctly classified as Class C.</li> </ul> Predicted Class A Predicted Class B Predicted Class C Actual Class A 15 (TP_A) 5 2 Actual Class B 4 20 (TP_B) 3 Actual Class C 2 8 25 (TP_C)"},{"location":"metric-definitions/#accuracy","title":"Accuracy","text":"<p>Returns the accuracy classification score. In <code>multi-class classification</code>, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true. Formula:</p>  accuracy = {(TP + TN) \\over (TP + TN + FP + FN)}   accuracy = {(TP + TN) \\over (TP + TN + FP + FN)}"},{"location":"metric-definitions/#precision","title":"Precision","text":"<p>Returns the precision classification score. In <code>multi-class classification</code>, returns the below 3 scores:</p> <ul> <li><code>micro</code>: Calculate metrics globally by counting the total true positives, false negatives and false positives.</li> <li><code>macro</code>: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</li> <li><code>weighted</code>: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.</li> </ul> <p>Formula:</p>  precision = {TP \\over (TP + FP)}   precision = {TP \\over (TP + FP)}"},{"location":"metric-definitions/#recall","title":"Recall","text":"<p>Returns the recall classification score. In <code>multi-class classification</code>, returns the below 3 scores:</p> <ul> <li><code>micro</code>: Calculate metrics globally by counting the total true positives, false negatives and false positives.</li> <li><code>macro</code>: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</li> <li><code>weighted</code>: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.</li> </ul> <p>Formula:</p>  recall = {TP \\over (TP + FN)}   recall = {TP \\over (TP + FN)}"},{"location":"metric-definitions/#f1-score","title":"F1 score","text":"<p>Returns the f1 classification score. In <code>multi-class classification</code>, returns the below 3 scores:</p> <ul> <li><code>micro</code>: Calculate metrics globally by counting the total true positives, false negatives and false positives.</li> <li><code>macro</code>: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</li> <li><code>weighted</code>: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.</li> </ul> <p>Formula:</p>  F1 = 2 * {(precision * recall) \\over (precision + recall)}   F1 = 2 * {(precision * recall) \\over (precision + recall)}"},{"location":"metric-definitions/#regression-evaluation-metrics","title":"Regression evaluation metrics","text":""},{"location":"metric-definitions/#r-squared","title":"R-Squared","text":"<p>In a regression model, R-Squared (also known as R2 or the coefficient of determination) is a statistical metric that quantifies how much of the variance in the dependent variable can be accounted for by the independent variable. R-squared, thus, displays how well the data match the regression model (the goodness of fit).</p> <p>Formula:</p>  R^2 = {SS_{regression} \\over SS_{total}}   R^2 = {SS_{regression} \\over SS_{total}}  <p>where:</p> <ul> <li>SS_{regression}SS_{regression} = sum of squares due to regression (explained sum of squares)</li> <li>SS_{total}SS_{total} = total sum of squares</li> </ul>"},{"location":"metric-definitions/#mean-squared-error","title":"Mean squared error","text":"<p>The degree of inaccuracy in statistical models is gauged by the mean squared error, or MSE. Between the observed and projected values, it evaluates the average squared difference. The MSE is equal to 0 when a model is error-free. As model error increases, its value increases.</p> <p>Formula:</p>  MSE = {\u03a3(y_{i}-y'_{i})^2 \\over n}   MSE = {\u03a3(y_{i}-y'_{i})^2 \\over n}  <p>where:</p> <ul> <li>y_{i}y_{i} = i_{th}i_{th} observation</li> <li>y'_{i}y'_{i} = corresponding predicted value</li> <li>nn = number of observations</li> </ul>"},{"location":"metric-definitions/#mean-absolute-error","title":"Mean absolute error","text":"<p>The mean absolute error of a model with respect to a test set is the mean of the absolute values of the individual prediction errors on over all instances in the test set. Each prediction error is the difference between the true value and the predicted value for the instance.</p> <p>Formula:</p>  MAE = {{{\u03a3^2}_{i=1}}|y_{i}-x_{i}| \\over n}   MAE = {{{\u03a3^2}_{i=1}}|y_{i}-x_{i}| \\over n}  <p>where:</p> <ul> <li>y_{i}y_{i} = prediction</li> <li>x_{i}x_{i} = true value</li> <li>nn = number of observations</li> </ul>"},{"location":"metric-definitions/#statistical-tests-and-techniques","title":"Statistical tests and techniques","text":""},{"location":"metric-definitions/#kolmogorov-smirnov-two-sample-test","title":"Kolmogorov-Smirnov Two Sample test","text":"<p>When there are two datasets then K-S two sample test can be used to test the agreement between their distributions. The null hypothesis states that there is no difference between the two distributions. Formula:</p>  D = Max|{F_a(X)-F_b(X)}|   D = Max|{F_a(X)-F_b(X)}|  <p>where:</p> <ul> <li>aa = observations from first dataset.</li> <li>bb = observations from second dataset.</li> <li>F_n(X)F_n(X) = observed cumulative frequency distribution of a random sample of n observations.</li> </ul>"},{"location":"metric-definitions/#chi-squared-test","title":"Chi-squared test","text":"<p>A chi-square test is a statistical test used to compare 2 datasets. The purpose of this test is to determine if a difference between data of 2 datasets is due to chance, or if it is due to a relationship between the variables you are studying. Formula:</p>  x^2 = \u03a3{(O_i - E_i)^2 \\over E_i}   x^2 = \u03a3{(O_i - E_i)^2 \\over E_i}  <p>where:</p> <ul> <li>x^2x^2 = chi-square</li> <li>O_iO_i = 1st dataset values</li> <li>E_iE_i = 2nd dataset values</li> </ul>"},{"location":"metric-definitions/#z-score-for-independent-proportions","title":"Z-score for independent proportions","text":"<p>The purpose of the z-test for independent proportions is to compare two independent datasets. Formula:</p>  Z = {p_1 - p_2 \\over \\sqrt{p'  q' ({1\\over n_1} + {1\\over n_2})}}   Z = {p_1 - p_2 \\over \\sqrt{p'  q' ({1\\over n_1} + {1\\over n_2})}}  <p>where:</p> <ul> <li>ZZ = Z-statistic which is compared to the standard normal deviate</li> <li>p_1 , p_2p_1 , p_2 = two datasets proportions</li> <li>p'p' = estimated true proportion under the null hypothesis</li> <li>q'q' = (1-p')(1-p')</li> <li>n_1 , n_2n_1 , n_2 = number of observations in two datasets</li> </ul>"},{"location":"metric-definitions/#wasserstein-distance","title":"Wasserstein distance","text":"<p>The Wasserstein distance is a metric to describe the distance between the distributions of 2 datasets. Formula:</p>  W = ({\\int_0^1}{{|{F_A}^{-1}(u) - {F_B}^{-1}(u)|}^2 du} )^{0.5}   W = ({\\int_0^1}{{|{F_A}^{-1}(u) - {F_B}^{-1}(u)|}^2 du} )^{0.5}  <p>where:</p> <ul> <li>WW = Wasserstein distance</li> <li>F_A , F_BF_A , F_B = corresponding cumulative distribution functions of two datasets</li> <li>{F_A}^{-1} , {F_B}^{-1}{F_A}^{-1} , {F_B}^{-1} = respective quantile functions</li> </ul>"},{"location":"metric-definitions/#jensenshannon-divergence","title":"Jensen\u2013Shannon divergence","text":"<p>The Jensen\u2013Shannon divergence is a method of measuring the similarity between two probability distributions. Formula:</p>  JS = 1/2 * KL(P || M) + 1/2 * KL(Q || M)   JS = 1/2 * KL(P || M) + 1/2 * KL(Q || M)  <p>where:</p> <ul> <li>JSJS = Jensen\u2013Shannon divergence</li> <li>KLKL = Kullback-Leibler divergence: \u2013 sum x\u2013 sum x in XX P(x)P(x) * log(Q(x) / P(x))log(Q(x) / P(x))</li> <li>P,QP,Q = distributions of 2 datasets</li> <li>MM = {1 \\over 2} * (P+Q){1 \\over 2} * (P+Q)</li> </ul>"},{"location":"metric-definitions/#machine-learning-models","title":"Machine learning models","text":""},{"location":"metric-definitions/#light-gradient-boosting-machine","title":"Light Gradient Boosting Machine","text":"<p>LightGBM is an open-source framework for gradient boosted machines. By default LightGBM will train a Gradient Boosted Decision Tree (GBDT), but it also supports random forests, Dropouts meet Multiple Additive Regression Trees (DART), and Gradient Based One-Side Sampling (Goss). The framework is fast and was designed for distributed training. It supports large-scale datasets and training on the GPU. LightGBM also provide highly optimised, scalable and fast implementations of gradient boosted machines (GBMs). The official documentation of LightGBM is accessible here.</p>"},{"location":"metric-definitions/#explainable-ai-models","title":"Explainable AI models","text":""},{"location":"metric-definitions/#local-interpretable-model-agnostic-explanations","title":"Local Interpretable Model-agnostic Explanations","text":"<p>LIME (Local Interpretable Model-agnostic Explanations), an explainable AI technique, aids in illuminating a machine learning model and making each prediction's particular implications understandable. The technique is appropriate for local explanations since it describes the classifier for a particular single instance. LIME modifies the input data to produce a succession of false data that only partially retain the original features. The original implementation along with documentation of LIME technique could be found in this repo.</p>"},{"location":"ml-monitoring/","title":"Understanding machine learning monitoring","text":"<p>Machine learning (ML) monitoring is a crucial process for ensuring the performance and reliability of ML models and systems. It involves tracking metrics, identifying issues, and improving overall performance. In this article, we will explore the key aspects of ML monitoring and its importance in today's data-driven world.</p>"},{"location":"ml-monitoring/#what-is-machine-learning-monitoring","title":"What is machine learning monitoring?","text":"<p>ML monitoring is the process of tracking the performance and behavior of ML models and systems. This includes monitoring metrics such as accuracy, precision, recall, and model performance over time. By monitoring these metrics, organizations can identify when a model is performing poorly or behaving unexpectedly and take action to correct it. Additionally, ML monitoring can be used to track the performance of different models and compare them to identify which model is performing best.</p>"},{"location":"ml-monitoring/#detecting-poor-performance","title":"Detecting poor performance","text":"<p>One of the key aspects of ML monitoring is being able to detect when a model is performing poorly or behaving unexpectedly. This can be done by setting up alerts on certain metrics or by monitoring for unexpected changes in the model's behavior. For example, if a model's accuracy suddenly drops, an alert can be triggered to notify the team responsible for maintaining the model. This allows them to quickly investigate and address the issue, minimizing the impact on the organization's operations.</p>"},{"location":"ml-monitoring/#monitoring-data-and-concept-drift","title":"Monitoring data and concept drift","text":"<p>Another important aspect of ML monitoring is the ability to detect and address data and concept drifting. Data drifting refers to the gradual change in the distribution or characteristics of the input data over time. As the data changes, the model's performance may decrease, which is why it is important to detect data drifting and retrain the model with new data. Concept drifting, on the other hand, happens when the statistical properties of the target variable, which the model is trying to predict, change over time. This can happen due to various reasons such as changes in the underlying data distribution, overfitting, or degradation of the model's parameters. To address these issues, beside drift detection, organizations can use techniques such as monitoring of the performance of the model over time, retraining the model regularly etc.</p>"},{"location":"ml-monitoring/#monitoring-input-data","title":"Monitoring input data","text":"<p>Another important aspect of ML monitoring is being able to track the input data that is being fed into the model. This can be used to identify any issues with the data, such as missing values, and to ensure that the data is being processed correctly. This helps to ensure that the model is making accurate predictions and that the data is being used effectively.</p>"},{"location":"ml-monitoring/#monitoring-infrastructure-and-resources","title":"Monitoring infrastructure and resources","text":"<p>Finally, it is important to monitor the infrastructure and resources that the ML models are running on. This includes monitoring things like CPU and memory usage, disk space, and network traffic. This helps to ensure that the models have the resources they need to perform well and to identify any potential bottlenecks that may be impacting performance. By monitoring the infrastructure, organizations can ensure that their models are running smoothly and can make adjustments as needed.</p>"},{"location":"ml-monitoring/#conclusion","title":"Conclusion","text":"<p>In conclusion, ML monitoring is an essential process that helps organizations ensure the performance and reliability of their ML models and systems. By tracking metrics, identifying issues, and monitoring input data and infrastructure, organizations can ensure that their models are delivering accurate and reliable results and that their systems are running smoothly. With the increasing reliance on data and ML, the importance of ML monitoring will only continue to grow.</p>"},{"location":"sdk-docs/","title":"SDK Documentation","text":"<p>This is the documentation for Whitebox's SDK. For an interactive experience, you can expirement with the SDK's Jupyter notebooks.</p>"},{"location":"sdk-docs/#models","title":"Models","text":"<p>create_model(name, type, prediction, labels=None, description=\"\")</p> <p>Creates a model in the database. This model works as placeholder for all the actual model's metadata.</p> Parameter Type Description name <code>str</code> The name of the model. type <code>str</code> The model's type. Possible values: <code>binary</code>, <code>multi_class</code>, <code>regression</code>. prediction <code>str</code> The prediction of the model. labels <code>Dict[str, int]</code> The model's labels. Defaults to <code>None</code>. description <code>str</code> The model's description. Defaults to an empty string <code>\"\"</code>. <p>Info</p> <p>Labels are not applicable ONLY in regression models.</p> <p>get_model(model_id)</p> <p>Fetches the model with the specified ID from the database.</p> Parameter Type Description model_id <code>str</code> The ID of the model. <p>delete_model(model_id)</p> <p>Deletes the model with the specified ID from the database.</p> Parameter Type Description model_id <code>str</code> The ID of the model."},{"location":"sdk-docs/#training-datasets","title":"Training Datasets","text":"<p>log_training_dataset(model_id, non_processed, processed)</p> <p>Inserts a set of dataset rows into the database. When the dataset rows are successfully saved, the pipeline for training the model is triggered. Then, the trained model is saved in the <code>/models/your_model's_id</code> folder of whitebox's root directory.</p> Parameter Type Description model_id <code>str</code> The ID of the model. non_processed <code>pd.DataFrame</code> The non processed training dataset. processed <code>pd.DataFrame</code> The processed training dataset. <p>Info</p> <p>The non processed and processed dataframes must have the same length.</p>"},{"location":"sdk-docs/#inferences","title":"Inferences","text":"<p>log_inferences(model_id, non_processed, processed, timestamps, actuals=None)</p> <p>Inserts a set of inference rows into the database.</p> Parameter Type Description model_id <code>str</code> The ID of the model. non_processed <code>pd.DataFrame</code> The non processed inferences. processed <code>pd.DataFrame</code> The processed inferences. timestamps <code>pd.Series</code> The timestamps for each inference row in the inference dataframes. actuals <code>pd.Series</code> The actuals for each inference row in the inference dataframes. Defaults to <code>None</code>. <p>Info</p> <p>The non processed and processed dataframes along with the timestamps and actuals series must ALL have the same length.</p> <p>get_xai_row(inference_row_id)</p> <p>Produces an explainability report for a specific inference row.</p> Parameter Type Description inference_row_id <code>str</code> The ID of the inference row."},{"location":"sdk-docs/#monitors","title":"Monitors","text":"<p>create_model_monitor(model_id, name, status, metric, severity, email, feature=None, lower_threshold=None)</p> <p>Creates a monitor for a specific metric.</p> Parameter Type Description model_id <code>str</code> The ID of the model. name <code>str</code> The name of the monitor. status <code>MonitorStatus</code> The status of the monitor. Possible values for <code>MonitorStatus</code>: <code>active</code>, <code>inactive</code>. metric <code>MonitorMetrics</code> The metric that will be monitored. Possible values for <code>MonitorMetrics</code>: <code>accuracy</code>, <code>precision</code>, <code>recall</code>, <code>f1</code>, <code>r_square</code>, <code>mean_squared_error</code>, <code>mean_absolute_error</code>, <code>data_drift</code>, <code>concept_drift</code>. severity <code>AlertSeverity</code> The severity of the alert the monitor produces. Possible values for <code>AlertSeverity</code>: <code>low</code>, <code>mid</code>, <code>high</code>. email <code>str</code> The email to which the alert will be sent. feature <code>str</code> The feature to be monitored. Defaults to <code>None</code>. lower_threshold <code>float</code> The threshold below which an alert will be produced. Defaults to <code>None</code>. <p>Note</p> <p>Some metrics like the data drift don't use a threshold so the feature that will be monitored should be inserted. In any case, both <code>feature</code> and <code>lower_threshold</code> can't be <code>None</code> at the same time.</p>"},{"location":"sdk-docs/#metrics","title":"Metrics","text":"<p>get_drifting_metrics(model_id)</p> <p>Fetches a model's drifting metric reports.</p> Parameter Type Description model_id <code>str</code> The ID of the model. <p>get_descriptive_statistics(model_id)</p> <p>Fetches a model's descriptive statistics reports.</p> Parameter Type Description model_id <code>str</code> The ID of the model. <p>get_performance_metrics(model_id)</p> <p>Fetches a model's performance metric reports.</p> Parameter Type Description model_id <code>str</code> The ID of the model."},{"location":"tutorial/installation/","title":"Installation","text":""},{"location":"tutorial/installation/#docker","title":"Docker","text":"<p>Install whitebox server and all of its dependencies using <code>docker-compose</code></p> <p>Copy the following code in a file named <code>docker-compose.yml</code>:</p> <pre><code>version: \"3.10\"\nname: Whitebox\nservices:\npostgres:\nimage: postgres:15\nrestart: unless-stopped\nenvironment:\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=postgres\n- POSTGRES_MULTIPLE_DATABASES=test # postgres db is created by default\nlogging:\noptions:\nmax-size: 10m\nmax-file: \"3\"\nports:\n- \"5432:5432\"\nvolumes:\n- wb_data:/var/lib/postgresql/data\nnetworks:\n- whitebox\nwhitebox:\nimage: sqdhub/whitebox:main\nrestart: unless-stopped\nenvironment:\n- APP_NAME=Whitebox | Docker\n- DATABASE_URL=postgresql://postgres:postgres@postgres:5432/postgres\n- SECRET_KEY=&lt;add_your_own&gt; # Optional, if not set the API key won't be encrypted\nports:\n- \"8000:8000\"\ndepends_on:\n- postgres\nnetworks:\n- whitebox\nvolumes:\nwb_data:\nnetworks:\nwhitebox:\nname: whitebox\n</code></pre> <p>With your terminal navigate to <code>docker-compose.yml</code>'s location and then run the following command:</p> <pre><code>$  docker-compose up\n</code></pre>"},{"location":"tutorial/installation/#kubernetes","title":"Kubernetes","text":"<p>You can also install Whitebox server and all of its dependencies in your k8s cluster using <code>helm</code>:</p> <pre><code>helm repo add squaredev https://chartmuseum.squaredev.io/\nhelm repo update\nhelm install whitebox squaredev/whitebox\n</code></pre>"},{"location":"tutorial/metrics/","title":"Metrics Calculation","text":"<p>Info</p> <p>The metrics are automatically calculated in a set interval for all models in the database.</p> <p>Depending on the data you have provided, the metrics calculation process will try to as many metrics possible for the specific model.</p>"},{"location":"tutorial/metrics/#metric-requirements","title":"Metric Requirements","text":"<p>All metrics require that inferences are provided for the specific model. If not, the whole process is skipped. Each metric though has different requirements that need to be fulfilled in order to be calculated. The requirements are listed in the following sections.</p>"},{"location":"tutorial/metrics/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>The descriptive statistics are calculated per feature on any given dataset.</p> Metric Type of data missing_count <code>numerical</code> &amp; <code>categorical</code> non_missing_count <code>numerical</code> &amp; <code>categorical</code> mean <code>numerical</code> minimum <code>numerical</code> maximum <code>numerical</code> sum <code>numerical</code> standard_deviation <code>numerical</code> variance <code>numerical</code>"},{"location":"tutorial/metrics/#drifting-metrics","title":"Drifting Metrics","text":"<p>The target of drift metrics is to calculate the data drift between 2 datasets. Currently supported drift types are:</p> <ul> <li>Data drift</li> <li>Concept drift</li> </ul> <p>Requirements:</p> <ul> <li>Inferences</li> <li>Training Dataset</li> </ul> <p>Note</p> <p>If actuals aren't provided for all inferences, then ONLY the inferences that have actuals will be used for the calculation of the drifting metrics.</p>"},{"location":"tutorial/metrics/#data-drift","title":"Data drift","text":"<p>An analysis happens comparing the current data to the reference data estimating the distributions of each feature in the two datasets. The schema of both datasets should be identical.</p> <p>Returns a drift summary of the following form:</p> <pre><code>{\n\"timestamp\": \"the timestamp of the report\",\n\"drift_summary\": {\n\"number_of_columns\": \"total number of dataset columns\",\n\"number_of_drifted_columns\": \"total number of drifted columns\",\n\"share_of_drifted_columns\": \"number_of_drifted_columns/number_of_columns\",\n\"dataset_drift\": \"Boolean based on the criteria below\",\n\"drift_by_columns\": {\n\"column1\": {\n\"column_name\": \"column1\",\n\"column_type\": \"the type of column (e.g. num)\",\n\"stattest_name\": \"the statistical test tha was used\",\n\"drift_score\": \"the drifting score based on the test\",\n\"drift_detected\": \"Boolean based on the criteria below\",\n\"threshold\": \"a float number based on the criteria below\"\n},\n\"column2\": {\"...\"}\n}\n}\n}\n</code></pre> <p>Logic to choose the appropriate statistical test is based on:</p> <ul> <li>feature type: categorical or numerical</li> <li>the number of observations in the reference dataset</li> <li>the number of unique values in the feature (n_unique)</li> </ul> <p>For small data with &lt;= 1000 observations in the reference dataset:</p> <ul> <li>For numerical features (n_unique &gt; 5): two-sample Kolmogorov-Smirnov test.</li> <li>For categorical features or numerical features with n_unique &lt;= 5: chi-squared test.</li> <li>For binary categorical features (n_unique &lt;= 2), we use the proportion difference test for independent samples based on Z-score.</li> </ul> <p>All tests use a 0.95 confidence level by default.</p> <p>For larger data with &gt; 1000 observations in the reference dataset:</p> <ul> <li>For numerical features (n_unique &gt; 5): Wasserstein Distance.</li> <li>For categorical features or numerical with n_unique &lt;= 5): Jensen\u2013Shannon divergence.</li> </ul> <p>All tests use a threshold = 0.1 by default.</p>"},{"location":"tutorial/metrics/#concept-drift","title":"Concept drift","text":"<p>An analysis happens comparing the current target feature to the reference target feature.</p> <p>Returns a concept drift summary of the following form:</p> <pre><code>{\n\"timestamp\": \"the timestamp of the report\",\n\"concept_drift_summary\": {\n\"column_name\": \"column1\",\n\"column_type\": \"the type of column (e.g. num)\",\n\"stattest_name\": \"the statistical test tha was used\",\n\"threshold\": \"threshold used based on criteria below\",\n\"drift_score\": \"the drifting score based on the test\",\n\"drift_detected\": \"Boolean based on the criteria below,\"\n}\n}\n</code></pre> <p>Logic to choose the appropriate statistical test is based on:</p> <ul> <li>the number of observations in the reference dataset</li> <li>the number of unique values in the target (n_unique)</li> </ul> <p>For small data with &lt;= 1000 observations in the reference dataset:</p> <ul> <li>For categorical target with n_unique &gt; 2: chi-squared test.</li> <li>For binary categorical target (n_unique &lt;= 2), we use the proportion difference test for independent samples based on Z-score.</li> </ul> <p>All tests use a 0.95 confidence level by default.</p> <p>For larger data with &gt; 1000 observations in the reference dataset we use Jensen\u2013Shannon divergence with a threshold = 0.1 .</p>"},{"location":"tutorial/metrics/#performance-evaluation-metrics","title":"Performance Evaluation Metrics","text":"<p>Requirements:</p> <ul> <li>Inferences</li> <li>Actuals for the inferences</li> </ul> <p>The target of evaluation metrics is to evaluate the quality of an machine learning model. Currently supported models are:</p> <ul> <li>Binary classification</li> <li>Multi-class classification</li> <li>Regression</li> </ul> Metric Supported model confusion matrix <code>binary classification</code> &amp; <code>multi-class classification</code> accuracy <code>binary classification</code> &amp; <code>multi-class classification</code> precision <code>binary classification</code> &amp; <code>multi-class classification</code> recall <code>binary classification</code> &amp; <code>multi-class classification</code> f1 score <code>binary classification</code> &amp; <code>multi-class classification</code> r_square <code>regression</code> mean_squared_error <code>regression</code> mean_absolute_error <code>regression</code>"},{"location":"tutorial/metrics/#explainability","title":"Explainability","text":"<p>Requirements:</p> <ul> <li>Inferences</li> </ul> <p>The target of the explainability feature is to provide a contribution score of each feature to each individual prediction, in a try to explain how the model concluded in the specific prediction. To achieve this we define 3 Levels of confidence in the explainability feature, based on how accessible or not is the client's input:</p> <ul> <li>Level-0: In this level a replacement model is trained in the same training data as client's model, and used for the explainability feature.</li> <li>Level-1 (pending): In this level a surrogate model is trained in a way of trying to achieve the same predictions as client's model, and used for the explainability feature.</li> <li>Level-2 (pending): Client's model is used for the explainability feature.</li> </ul>"},{"location":"tutorial/metrics/#level-0-confidence","title":"Level-0 confidence","text":"<p>At this level a replacement model is trained in the same training data as client's model, and used for the explainability feature. Below there is a table of used models per machine learning task.</p> Model Task LightGBM <code>binary classification</code> &amp; <code>multi-class classification</code> &amp; <code>regression</code> <p>The fine tuning of models, through a hyper-parameters exploration is a pending task for now.</p> <p>The trained model along with the inference data are used as input in LIME library, and the below report is provided - presenting the contribution of each feature in a specific prediction (the report includes all the feature contribution score to an descending order based on the absolute score value):</p> <pre><code>{\n\"feature1\": \"contribution score\",\n\"feature2\": \"contribution score\",\n\"feature3\": \"contribution score\"\n}\n</code></pre>"},{"location":"tutorial/metrics/#level-1-confidence","title":"Level-1 confidence","text":"<p>Coming soon</p>"},{"location":"tutorial/metrics/#level-2-confidence","title":"Level-2 confidence","text":"<p>Coming soon</p>"},{"location":"tutorial/monitors_alerts/","title":"Monitors and Alerts","text":""},{"location":"tutorial/monitors_alerts/#monitors","title":"Monitors","text":"<p>You can create a monitor in whitebox so that alert are created automaticaly when some value is out of bounds. Here is an example:</p> <pre><code>from whitebox import Whitebox, MonitorStatus, MonitorMetrics, AlertSeverity\nwb = Whitebox(host=\"127.0.0.1:8000\", api_key=\"some_api_key\")\nmodel_monitor = wb.create_model_monitor(\nmodel_id=\"mock_model_id\",\nname=\"test\",\nstatus=MonitorStatus.active,\nmetric=MonitorMetrics.accuracy,\nseverity=AlertSeverity.high,\nemail=\"jackie.chan@somemail.io\",\nlower_threshold=0.7\n)\n</code></pre>"},{"location":"tutorial/monitors_alerts/#alerts","title":"Alerts","text":"<p>Once the metrics reports have been produced, the monitoring alert pipeline is triggered. This means that if you have created any model monitors for a specific metric, alerts will be created if certain criteria are met, based on the thresholds and the monitor types you have specified.</p>"},{"location":"tutorial/sdk/","title":"Using Whitebox SDK","text":""},{"location":"tutorial/sdk/#installation","title":"Installation","text":"<p>Installing Whitebox is a pretty easy job. Just install it like any other python package.</p> <p>Install the SDK with <code>pip</code>:</p> <pre><code>$  pip install whitebox-sdk\n</code></pre> <p>All the required packages will be automatically installed!</p> <p>Now you're good to go!</p>"},{"location":"tutorial/sdk/#initial-setup","title":"Initial Setup","text":"<p>In order to run Whitebox, you will need the application's API key. This key will be produced for you during the initial run of the Uvicorn live server. Assuming you run the server with docker compose (you can find more in the install page of the tutorial), you will see the following output:</p> <pre><code>$ docker compose up\n\n...\n&lt;span style=\"color: green;\"&gt;INFO&lt;/span&gt;: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n&lt;span style=\"color: green;\"&gt;INFO&lt;/span&gt;: Started reloader process [4450] using StatReload\n&lt;span style=\"color: green;\"&gt;INFO&lt;/span&gt;: Started server process [4452]\n&lt;span style=\"color: green;\"&gt;INFO&lt;/span&gt;: Waiting for application startup.\n&lt;span style=\"color: green;\"&gt;INFO&lt;/span&gt;: Created username: admin, API key: some_api_key\n&lt;span style=\"color: green;\"&gt;INFO&lt;/span&gt;: Application startup complete.\n...\n</code></pre> <p>Info</p> <p>Keep this API key somewhere safe!</p> <p>If you lose it, you will need to delete the admin user in your database and re-run the live serve to produce a new key!</p> <p>After you get the API key, all you have to do is create an instance of the Whitebox class adding your host and API key as parameters:</p> <pre><code>from whitebox import Whitebox\nwb = Whitebox(host=\"http://127.0.0.1:8000\", api_key=\"some_api_key\")\n</code></pre> <p>Now you're ready to start using Whitebox!</p>"},{"location":"tutorial/sdk/#models","title":"Models","text":""},{"location":"tutorial/sdk/#creating-a-model","title":"Creating a Model","text":"<p>In order to start adding training datasets and inferences, you first need to create a model.</p> <p>Let's create a sample model:</p> <pre><code>wb.create_model(\nname=\"Model 1\",\ntype=\"binary\",\nlabels={\n'additionalProp1': 0,\n'additionalProp2': 1\n},\nprediction=\"target\"\n)\n</code></pre> <p>For more details about the schema accepted property types visit the Models section in the SDK documentation.</p>"},{"location":"tutorial/sdk/#fetching-a-model","title":"Fetching a Model","text":"<p>Getting a model from the database is as easy as it sounds. You'll just need the <code>model_id</code>:</p> <pre><code>wb.get_model(\"some_model_id\")\n</code></pre>"},{"location":"tutorial/sdk/#deleting-a-model","title":"Deleting a model","text":"<p>Deleting a model is as easy as fetching a model. Just use the <code>model_id</code>:</p> <pre><code>wb.delete_model(\"some_model_id\")\n</code></pre> <p>Warning</p> <p>You will have to be extra careful when deleting a model because all datasets, inferences, monitors and literally everything will be deleted from the database along with the model itself!</p>"},{"location":"tutorial/sdk/#loading-training-datasets","title":"Loading Training Datasets","text":"<p>Once you have created a model you can start loading your data. Let's start with the training dataset!</p> <p>In our example we will create a <code>pd.DataFrame</code> from a <code>.csv</code> file. Of course you can use any method you like to create your <code>pd.DataFrame</code> as long as your non-processed and processed datasets have the same amount of rows (a.k.a. the same length) and there are more than one rows!</p> <pre><code>import pandas as pd\nnon_processed_df = pd.read_csv(\"path/to/file/non_processed_data.csv\")\nprocessed_df = pd.read_csv(\"path/to/file/processed_data.csv\")\nwb.log_training_dataset(\nmodel_id=\"some_model_id\",\nnon_processed=non_processed_df,\nprocessed=processed_df\n)\n</code></pre> <p>Note</p> <p>When your training dataset is saved in the database, the model training process will begin excecuting, based on this dataset and the model it's associated with. That's why you need to load all the rows of your training dataset in the same batch.</p>"},{"location":"tutorial/sdk/#loading-inferences","title":"Loading Inferences","text":"<p>To load your inferences you have to follow the exact same procedure as with the training datasets. The only difference is that you need to provide a <code>pd.Series</code> with the timestamps and (optionally) a <code>pd.Series</code> with the actuals, whose indices should match the ones in the non-processed and processed <code>pd.DataFrames</code>.</p> <p>In our example let's assume that both the non-processed and processed <code>pd.DataFrames</code> have 10 rows each:</p> <pre><code>import pandas as pd\nnon_processed_df = pd.read_csv(\"path/to/file/non_processed_data.csv\")\nprocessed_df = pd.read_csv(\"path/to/file/processed_data.csv\")\n# Timestamps and actuals should have a length of 10\ntimestamps = pd.Series([\"2022-12-22T12:13:27.879738\"] * 10)\nactuals = pd.Series([0, 1, 1, 1, 0, 0, 1, 1, 0, 0])\nwb.log_inferences(\nmodel_id=\"some_model_id\",\nnon_processed=non_processed_df,\nprocessed=processed_df,\ntimestamps=timestamps,\nactuals=actuals\n)\n</code></pre> <p>Warning</p> <p>Make sure you add the actuals if you already know them, because as of now, there's no ability to add them at a later time by updating the inference rows.</p>"}]}